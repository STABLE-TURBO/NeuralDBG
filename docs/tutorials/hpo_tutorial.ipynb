{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization (HPO) Tutorial\n",
    "\n",
    "This tutorial teaches you how to optimize neural network hyperparameters using Neural DSL's built-in HPO capabilities.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. HPO syntax and configuration\n",
    "2. Optimizing layer parameters\n",
    "3. Optimizing optimizer settings\n",
    "4. Running HPO trials\n",
    "5. Analyzing and applying results\n",
    "6. Multi-framework HPO (TensorFlow and PyTorch)\n",
    "\n",
    "**Time:** ~30 minutes  \n",
    "**Level:** Intermediate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is HPO?\n",
    "\n",
    "Hyperparameter optimization automatically finds the best configuration for your model:\n",
    "- **Layer sizes** (number of units, filters)\n",
    "- **Learning rate** and optimizer settings\n",
    "- **Dropout rates** for regularization\n",
    "- **Architecture choices** (kernel sizes, activations)\n",
    "\n",
    "Instead of manual trial-and-error, HPO systematically searches the space of possibilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Neural DSL with HPO support\n",
    "!pip install neural-dsl optuna tensorflow\n",
    "\n",
    "import neural\n",
    "print(f\"Neural DSL version: {neural.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Basic HPO - Optimizing Dense Units\n",
    "\n",
    "Let's start with a simple example: finding the best number of units in a dense layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model with HPO for Dense units\n",
    "basic_hpo_model = \"\"\"\n",
    "network BasicHPO {\n",
    "  input: (28, 28, 1)\n",
    "  \n",
    "  layers:\n",
    "    Flatten()\n",
    "    # Try different numbers of units: 64, 128, 256, or 512\n",
    "    Dense(units=HPO(choice(64, 128, 256, 512)), activation=\"relu\")\n",
    "    Dropout(rate=0.5)\n",
    "    Output(units=10, activation=\"softmax\")\n",
    "  \n",
    "  loss: \"sparse_categorical_crossentropy\"\n",
    "  optimizer: Adam(learning_rate=0.001)\n",
    "  metrics: [\"accuracy\"]\n",
    "  \n",
    "  train {\n",
    "    epochs: 5\n",
    "    batch_size: 64\n",
    "    validation_split: 0.2\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "with open('basic_hpo.neural', 'w') as f:\n",
    "    f.write(basic_hpo_model)\n",
    "\n",
    "print(\"‚úÖ Model with HPO saved to 'basic_hpo.neural'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HPO Syntax Explained\n",
    "\n",
    "```yaml\n",
    "Dense(units=HPO(choice(64, 128, 256, 512)))\n",
    "```\n",
    "\n",
    "- **`HPO(...)`**: Marks parameter for optimization\n",
    "- **`choice(64, 128, 256, 512)`**: Try these discrete values\n",
    "- HPO will test each value and find the best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run HPO (this will take a few minutes)\n",
    "!neural hpo basic_hpo.neural --backend tensorflow --trials 4 --output optimized_basic.neural\n",
    "\n",
    "print(\"\\n‚úÖ HPO completed! Best configuration saved to 'optimized_basic.neural'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Multiple Parameter HPO\n",
    "\n",
    "Let's optimize multiple parameters simultaneously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model with multiple HPO parameters\n",
    "multi_hpo_model = \"\"\"\n",
    "network MultiHPO {\n",
    "  input: (28, 28, 1)\n",
    "  \n",
    "  layers:\n",
    "    # Optimize Conv2D filters: try 16, 32, or 64\n",
    "    Conv2D(\n",
    "      filters=HPO(choice(16, 32, 64)),\n",
    "      kernel_size=(3, 3),\n",
    "      activation=\"relu\"\n",
    "    )\n",
    "    MaxPooling2D(pool_size=(2, 2))\n",
    "    \n",
    "    Flatten()\n",
    "    \n",
    "    # Optimize Dense units: range from 64 to 256, step by 32\n",
    "    Dense(\n",
    "      units=HPO(range(64, 256, step=32)),\n",
    "      activation=\"relu\"\n",
    "    )\n",
    "    \n",
    "    # Optimize dropout rate: try values between 0.2 and 0.7\n",
    "    Dropout(rate=HPO(range(0.2, 0.7, step=0.1)))\n",
    "    \n",
    "    Output(units=10, activation=\"softmax\")\n",
    "  \n",
    "  loss: \"sparse_categorical_crossentropy\"\n",
    "  optimizer: Adam(learning_rate=0.001)\n",
    "  metrics: [\"accuracy\"]\n",
    "  \n",
    "  train {\n",
    "    epochs: 5\n",
    "    batch_size: 64\n",
    "    validation_split: 0.2\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "with open('multi_hpo.neural', 'w') as f:\n",
    "    f.write(multi_hpo_model)\n",
    "\n",
    "print(\"‚úÖ Multi-parameter HPO model saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HPO Functions Reference\n",
    "\n",
    "| Function | Usage | Example |\n",
    "|----------|-------|--------|\n",
    "| `choice(...)` | Discrete values | `HPO(choice(32, 64, 128))` |\n",
    "| `range(min, max, step)` | Integer range | `HPO(range(10, 100, step=10))` |\n",
    "| `log_range(min, max)` | Log-scale range | `HPO(log_range(1e-5, 1e-2))` |\n",
    "\n",
    "**When to use each:**\n",
    "- `choice`: When you have specific values to try\n",
    "- `range`: For integer parameters (units, filters)\n",
    "- `log_range`: For learning rates (vary across orders of magnitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run multi-parameter HPO with more trials\n",
    "!neural hpo multi_hpo.neural --backend tensorflow --trials 10 --output optimized_multi.neural\n",
    "\n",
    "print(\"\\n‚úÖ Multi-parameter HPO completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Optimizing Learning Rate\n",
    "\n",
    "Learning rate is one of the most important hyperparameters. Let's optimize it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model with learning rate HPO\n",
    "lr_hpo_model = \"\"\"\n",
    "network LearningRateHPO {\n",
    "  input: (28, 28, 1)\n",
    "  \n",
    "  layers:\n",
    "    Conv2D(filters=32, kernel_size=(3, 3), activation=\"relu\")\n",
    "    MaxPooling2D(pool_size=(2, 2))\n",
    "    Flatten()\n",
    "    Dense(units=128, activation=\"relu\")\n",
    "    Dropout(rate=0.5)\n",
    "    Output(units=10, activation=\"softmax\")\n",
    "  \n",
    "  loss: \"sparse_categorical_crossentropy\"\n",
    "  \n",
    "  # Use log_range for learning rate\n",
    "  # Searches from 0.00001 to 0.01 on log scale\n",
    "  optimizer: Adam(learning_rate=HPO(log_range(1e-5, 1e-2)))\n",
    "  \n",
    "  metrics: [\"accuracy\"]\n",
    "  \n",
    "  train {\n",
    "    epochs: 5\n",
    "    batch_size: 64\n",
    "    validation_split: 0.2\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "with open('lr_hpo.neural', 'w') as f:\n",
    "    f.write(lr_hpo_model)\n",
    "\n",
    "print(\"‚úÖ Learning rate HPO model saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why log_range for Learning Rate?\n",
    "\n",
    "Learning rates vary across orders of magnitude:\n",
    "- 0.00001 (1e-5)\n",
    "- 0.0001 (1e-4)\n",
    "- 0.001 (1e-3)\n",
    "- 0.01 (1e-2)\n",
    "\n",
    "`log_range` samples uniformly on log scale, ensuring we explore all orders of magnitude equally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run learning rate optimization\n",
    "!neural hpo lr_hpo.neural --backend tensorflow --trials 8 --output optimized_lr.neural\n",
    "\n",
    "print(\"\\n‚úÖ Learning rate optimization completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 4: Comprehensive HPO\n",
    "\n",
    "Let's put it all together and optimize many parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive HPO model\n",
    "comprehensive_hpo = \"\"\"\n",
    "network ComprehensiveHPO {\n",
    "  input: (28, 28, 1)\n",
    "  \n",
    "  layers:\n",
    "    # First conv block\n",
    "    Conv2D(\n",
    "      filters=HPO(choice(16, 32, 64)),\n",
    "      kernel_size=(3, 3),\n",
    "      activation=\"relu\"\n",
    "    )\n",
    "    MaxPooling2D(pool_size=(2, 2))\n",
    "    \n",
    "    # Second conv block\n",
    "    Conv2D(\n",
    "      filters=HPO(choice(32, 64, 128)),\n",
    "      kernel_size=(3, 3),\n",
    "      activation=\"relu\"\n",
    "    )\n",
    "    MaxPooling2D(pool_size=(2, 2))\n",
    "    \n",
    "    Flatten()\n",
    "    \n",
    "    # Dense layers\n",
    "    Dense(\n",
    "      units=HPO(choice(64, 128, 256, 512)),\n",
    "      activation=\"relu\"\n",
    "    )\n",
    "    Dropout(rate=HPO(range(0.3, 0.7, step=0.1)))\n",
    "    \n",
    "    Dense(\n",
    "      units=HPO(choice(32, 64, 128)),\n",
    "      activation=\"relu\"\n",
    "    )\n",
    "    Dropout(rate=HPO(range(0.2, 0.5, step=0.1)))\n",
    "    \n",
    "    Output(units=10, activation=\"softmax\")\n",
    "  \n",
    "  loss: \"sparse_categorical_crossentropy\"\n",
    "  \n",
    "  # Optimize learning rate and batch size\n",
    "  optimizer: Adam(learning_rate=HPO(log_range(1e-5, 1e-2)))\n",
    "  metrics: [\"accuracy\"]\n",
    "  \n",
    "  train {\n",
    "    epochs: 5\n",
    "    # Can also optimize batch size (power of 2)\n",
    "    batch_size: HPO(choice(32, 64, 128))\n",
    "    validation_split: 0.2\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "with open('comprehensive_hpo.neural', 'w') as f:\n",
    "    f.write(comprehensive_hpo)\n",
    "\n",
    "print(\"‚úÖ Comprehensive HPO model saved\")\n",
    "print(\"\\nOptimizing: filters, units, dropout rates, learning rate, batch size\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note on Search Space\n",
    "\n",
    "This model has a large search space:\n",
    "- Conv1 filters: 3 options\n",
    "- Conv2 filters: 3 options  \n",
    "- Dense1 units: 4 options\n",
    "- Dense2 units: 3 options\n",
    "- Dropout rates: Multiple options\n",
    "- Learning rate: Continuous\n",
    "- Batch size: 3 options\n",
    "\n",
    "**Recommendation:** Start with 20-50 trials, increase if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run comprehensive HPO (will take longer)\n",
    "# Adjust --trials based on time available\n",
    "!neural hpo comprehensive_hpo.neural --backend tensorflow --trials 20 --output best_model.neural\n",
    "\n",
    "print(\"\\n‚úÖ Comprehensive HPO completed!\")\n",
    "print(\"Best configuration saved to 'best_model.neural'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing HPO Results\n",
    "\n",
    "Let's examine the optimized model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read optimized model\n",
    "with open('best_model.neural', 'r') as f:\n",
    "    optimized_model = f.read()\n",
    "\n",
    "print(\"Optimized Model Configuration:\")\n",
    "print(\"=\" * 50)\n",
    "print(optimized_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Before and After\n",
    "\n",
    "HPO replaces `HPO(...)` expressions with best values:\n",
    "\n",
    "**Before:**\n",
    "```yaml\n",
    "Dense(units=HPO(choice(64, 128, 256, 512)))\n",
    "optimizer: Adam(learning_rate=HPO(log_range(1e-5, 1e-2)))\n",
    "```\n",
    "\n",
    "**After:**\n",
    "```yaml\n",
    "Dense(units=256)  # Best value found\n",
    "optimizer: Adam(learning_rate=0.0003)  # Best learning rate\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Framework HPO\n",
    "\n",
    "Neural DSL's HPO works across TensorFlow and PyTorch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run HPO with PyTorch backend\n",
    "!pip install torch torchvision\n",
    "\n",
    "# Same model, different backend\n",
    "!neural hpo comprehensive_hpo.neural --backend pytorch --trials 10 --output best_model_pytorch.neural\n",
    "\n",
    "print(\"\\n‚úÖ PyTorch HPO completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HPO Best Practices\n",
    "\n",
    "### 1. Start Small\n",
    "```yaml\n",
    "# ‚ùå Too many parameters at once\n",
    "Dense(units=HPO(range(10, 1000, step=10)))  # 100 options!\n",
    "\n",
    "# ‚úÖ Start with discrete choices\n",
    "Dense(units=HPO(choice(64, 128, 256)))  # 3 options\n",
    "```\n",
    "\n",
    "### 2. Use Appropriate Ranges\n",
    "```yaml\n",
    "# ‚úÖ Learning rate: log scale\n",
    "optimizer: Adam(learning_rate=HPO(log_range(1e-5, 1e-2)))\n",
    "\n",
    "# ‚úÖ Units: linear range\n",
    "Dense(units=HPO(range(32, 512, step=32)))\n",
    "\n",
    "# ‚úÖ Dropout: small steps\n",
    "Dropout(rate=HPO(range(0.2, 0.8, step=0.1)))\n",
    "```\n",
    "\n",
    "### 3. Prioritize Important Parameters\n",
    "\n",
    "Most impact:\n",
    "1. Learning rate\n",
    "2. Network architecture (layers, units)\n",
    "3. Regularization (dropout)\n",
    "\n",
    "Less impact:\n",
    "- Batch size (try 32, 64, 128)\n",
    "- Minor architectural details\n",
    "\n",
    "### 4. Use Early Stopping\n",
    "```yaml\n",
    "train {\n",
    "  epochs: 20\n",
    "  early_stopping: 5  # Stop if no improvement\n",
    "}\n",
    "```\n",
    "\n",
    "### 5. Parallel Trials\n",
    "```bash\n",
    "# Speed up HPO with parallel execution\n",
    "neural hpo model.neural --parallel 4\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common HPO Patterns\n",
    "\n",
    "### Pattern 1: Layer Size Progression\n",
    "```yaml\n",
    "# Optimize first layer, scale others\n",
    "Dense(units=HPO(choice(64, 128, 256)))  # Optimize\n",
    "Dense(units=64)  # Half of first layer (manual)\n",
    "```\n",
    "\n",
    "### Pattern 2: Learning Rate Schedule\n",
    "```yaml\n",
    "optimizer: Adam(\n",
    "  learning_rate=HPO(log_range(1e-5, 1e-2))\n",
    ")\n",
    "lr_schedule: ExponentialDecay(\n",
    "  initial_lr=0.001,\n",
    "  decay_rate=HPO(range(0.9, 0.99, step=0.01))\n",
    ")\n",
    "```\n",
    "\n",
    "### Pattern 3: Architecture Search\n",
    "```yaml\n",
    "# Optimize number of conv blocks\n",
    "Conv2D(filters=HPO(choice(16, 32, 64)))\n",
    "MaxPooling2D(pool_size=(2, 2))\n",
    "Conv2D(filters=HPO(choice(32, 64, 128)))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting HPO\n",
    "\n",
    "### Problem: HPO takes too long\n",
    "**Solution:**\n",
    "- Reduce `--trials`\n",
    "- Reduce `epochs` in train block\n",
    "- Use smaller search spaces\n",
    "- Enable `--parallel`\n",
    "\n",
    "### Problem: No improvement in trials\n",
    "**Solution:**\n",
    "- Widen search ranges\n",
    "- Check data preprocessing\n",
    "- Verify model architecture\n",
    "- Increase epochs per trial\n",
    "\n",
    "### Problem: Memory errors\n",
    "**Solution:**\n",
    "- Reduce max batch size in HPO\n",
    "- Limit max units/filters\n",
    "- Use gradient accumulation\n",
    "\n",
    "### Problem: Unstable training\n",
    "**Solution:**\n",
    "- Narrow learning rate range\n",
    "- Add gradient clipping\n",
    "- Use batch normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You learned:\n",
    "\n",
    "‚úÖ HPO syntax: `HPO(choice(...))`, `HPO(range(...))`, `HPO(log_range(...))`  \n",
    "‚úÖ Optimizing layer parameters (units, filters, dropout)  \n",
    "‚úÖ Optimizing optimizer settings (learning rate)  \n",
    "‚úÖ Running HPO trials with `neural hpo`  \n",
    "‚úÖ Analyzing and applying results  \n",
    "‚úÖ Multi-framework HPO (TensorFlow and PyTorch)  \n",
    "‚úÖ Best practices and common patterns  \n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- **[Advanced Architectures Tutorial](advanced_architectures.ipynb)** - Complex models\n",
    "- **[Cloud Tutorial](cloud_tutorial.ipynb)** - HPO in the cloud\n",
    "- **[Production Deployment](deployment_tutorial.ipynb)** - Deploy optimized models\n",
    "\n",
    "## Resources\n",
    "\n",
    "- **[HPO Guide](../examples/hpo_guide.md)** - Detailed HPO documentation\n",
    "- **[Examples](../../examples/)** - More HPO examples\n",
    "- **[Discord](https://discord.gg/KFku4KvS)** - Get help with HPO\n",
    "\n",
    "Happy optimizing! üéØ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
