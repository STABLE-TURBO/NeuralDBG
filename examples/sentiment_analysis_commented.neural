# ============================================================================
# Sentiment Analysis with LSTM - Detailed Example
# ============================================================================
# This example demonstrates a recurrent neural network (RNN) for sentiment
# analysis, classifying text as positive or negative.
#
# Model Architecture:
#   - Input: Sequences of word indices (padded to fixed length)
#   - Embedding layer: Maps word indices to dense vectors
#   - Bidirectional LSTM: Captures sequential dependencies
#   - Dropout and Dense layers for classification
#   - Binary output: Positive (1) or Negative (0)
#
# Key Concepts Demonstrated:
#   - Embedding layers for text representation
#   - Recurrent layers (LSTM) for sequential data
#   - Bidirectional processing for better context
#   - Binary classification with sigmoid activation
# ============================================================================

network SentimentAnalyzer {
  # --------------------------------------------------------------------
  # INPUT SPECIFICATION
  # --------------------------------------------------------------------
  # Define input shape: (sequence_length,)
  # For text: sequence of word indices
  # Example: [42, 156, 8, 923, ...] (max 100 words)
  # Shorter sequences are padded, longer are truncated
  input: (100,)
  
  # --------------------------------------------------------------------
  # NETWORK LAYERS
  # --------------------------------------------------------------------
  layers:
    # Embedding Layer
    # ---------------
    # Converts word indices to dense vectors
    # - input_dim=10000: Vocabulary size (10,000 unique words)
    #   This should match your data preprocessing
    # - output_dim=128: Size of word embedding vectors
    #   Each word is represented as a 128-dimensional vector
    # 
    # How it works:
    #   - Input: [5, 42, 8] (word indices)
    #   - Output: [[0.1, 0.3, ...], [0.5, 0.2, ...], ...]
    #             (3 vectors of 128 dimensions each)
    #
    # Benefits:
    #   - Captures semantic relationships (similar words → similar vectors)
    #   - Learnable: Embeddings are updated during training
    #   - Efficient: Dense representation instead of one-hot encoding
    #
    # Output shape: (100, 128) - 100 words, 128-dim vectors
    Embedding(input_dim=10000, output_dim=128)
    
    # Bidirectional LSTM Layer
    # -------------------------
    # LSTM (Long Short-Term Memory) processes sequences
    # Bidirectional: Processes text both forward and backward
    #
    # - units=64: Number of LSTM units (memory cells)
    # - return_sequences=False: Only return final output
    #   (Set True if stacking multiple LSTM layers)
    #
    # How LSTM works:
    #   - Maintains hidden state across sequence
    #   - Gates control information flow (forget, input, output)
    #   - Captures long-range dependencies
    #
    # Bidirectional processing:
    #   - Forward: "The movie was great" → positive sentiment
    #   - Backward: "great was movie The" → same meaning
    #   - Combines both directions for better understanding
    #
    # Output shape: (128,) - 64 units × 2 directions
    LSTM(units=64, return_sequences=False)
    
    # First Dropout Layer
    # -------------------
    # Prevents overfitting by randomly dropping connections
    # - rate=0.5: Drop 50% of neurons during training
    # Helps model generalize to unseen text
    Dropout(rate=0.5)
    
    # First Dense Layer
    # -----------------
    # Learns complex patterns from LSTM output
    # - units=64: 64 neurons for feature combinations
    # - activation="relu": Non-linear activation
    # Extracts high-level sentiment indicators
    Dense(units=64, activation="relu")
    
    # Second Dropout Layer
    # --------------------
    # Additional regularization before final prediction
    # - rate=0.3: Lower rate than first dropout
    Dropout(rate=0.3)
    
    # Output Layer
    # ------------
    # Binary classification: positive (1) or negative (0)
    # - units=1: Single output neuron
    # - activation="sigmoid": Squashes output to [0, 1]
    #
    # Interpretation:
    #   - Output > 0.5: Positive sentiment
    #   - Output < 0.5: Negative sentiment
    #   - Confidence: Distance from 0.5
    Output(units=1, activation="sigmoid")
  
  # --------------------------------------------------------------------
  # TRAINING CONFIGURATION
  # --------------------------------------------------------------------
  
  # Loss Function
  # -------------
  # binary_crossentropy: Standard loss for binary classification
  # Penalizes confident wrong predictions more heavily
  loss: "binary_crossentropy"
  
  # Optimizer
  # ---------
  # Adam with lower learning rate for RNN stability
  # - learning_rate=0.0001: Smaller steps prevent oscillation
  # RNNs are more sensitive to learning rate than CNNs
  optimizer: Adam(learning_rate=0.0001)
  
  # Evaluation Metrics
  # ------------------
  # Track multiple metrics for comprehensive evaluation
  # - accuracy: Overall correctness
  # - precision: True positives / All predicted positives
  # - recall: True positives / All actual positives
  # - auc: Area under ROC curve (threshold-independent metric)
  metrics: ["accuracy", "precision", "recall", "auc"]
  
  # Training Parameters
  # -------------------
  train {
    # epochs: RNNs often need more epochs than CNNs
    # 20 epochs allows sufficient time to learn patterns
    epochs: 20
    
    # batch_size: Larger batches for RNN stability
    # 128 provides good gradient estimates
    batch_size: 128
    
    # validation_split: 20% for monitoring generalization
    validation_split: 0.2
  }
}

# ============================================================================
# DATA PREPROCESSING GUIDE
# ============================================================================
#
# Before using this model, preprocess your text data:
#
# 1. Tokenization:
#    - Convert text to sequences of word indices
#    - Build vocabulary (10,000 most common words)
#    - Handle unknown words (use special token)
#
# 2. Padding:
#    - Pad sequences to length 100
#    - Use post-padding (add zeros at end)
#
# 3. Example with Keras:
#    ```python
#    from tensorflow.keras.preprocessing.text import Tokenizer
#    from tensorflow.keras.preprocessing.sequence import pad_sequences
#    
#    # Fit tokenizer
#    tokenizer = Tokenizer(num_words=10000)
#    tokenizer.fit_on_texts(texts)
#    
#    # Convert to sequences
#    sequences = tokenizer.texts_to_sequences(texts)
#    
#    # Pad to fixed length
#    X = pad_sequences(sequences, maxlen=100, padding='post')
#    ```
#
# ============================================================================
# USAGE EXAMPLES
# ============================================================================
#
# Compile to TensorFlow:
#   neural compile examples/sentiment_analysis_commented.neural --backend tensorflow
#
# Compile to PyTorch:
#   neural compile examples/sentiment_analysis_commented.neural --backend pytorch
#
# Visualize architecture:
#   neural visualize examples/sentiment_analysis_commented.neural
#
# Debug training:
#   neural debug examples/sentiment_analysis_commented.neural
#
# ============================================================================
# HYPERPARAMETER TUNING SUGGESTIONS
# ============================================================================
#
# If model performance is not satisfactory, try:
#
# 1. Embedding dimensions:
#    - Smaller (64): Faster training, less expressive
#    - Larger (256): More expressive, risk of overfitting
#
# 2. LSTM units:
#    - Fewer (32): Simpler model, less prone to overfitting
#    - More (128): Captures more complex patterns
#
# 3. Learning rate:
#    - Too high: Loss oscillates or explodes
#    - Too low: Very slow convergence
#    - Recommended range: 0.00001 to 0.001
#
# 4. Dropout rates:
#    - Lower (0.2): Less regularization, better training accuracy
#    - Higher (0.7): More regularization, better generalization
#
# 5. Sequence length:
#    - Shorter (50): Faster, less context
#    - Longer (200): More context, slower training
#
# Use HPO for automatic tuning:
#   ```yaml
#   Embedding(input_dim=10000, output_dim=HPO(choice(64, 128, 256)))
#   LSTM(units=HPO(choice(32, 64, 128)))
#   optimizer: Adam(learning_rate=HPO(log_range(1e-5, 1e-3)))
#   ```
#
# ============================================================================
# EXPECTED PERFORMANCE
# ============================================================================
#
# On IMDB sentiment dataset:
#   - Training accuracy: ~90-95%
#   - Validation accuracy: ~85-90%
#   - Training time: 10-15 minutes per epoch on CPU
#   - Convergence: Typically by epoch 10-15
#
# Warning signs:
#   - Training accuracy >> Validation accuracy: Overfitting
#     → Increase dropout, reduce model size
#   - Both accuracies low: Underfitting
#     → Increase model capacity, train longer
#   - Loss not decreasing: Learning rate issues
#     → Try different learning rate
#
# ============================================================================
# COMMON PITFALLS
# ============================================================================
#
# 1. Vocabulary mismatch:
#    - Ensure input_dim matches tokenizer vocab size
#    - Solution: Set num_words=10000 in tokenizer
#
# 2. Sequence length mismatch:
#    - Input shape must match padded sequence length
#    - Solution: pad_sequences(maxlen=100)
#
# 3. Label format:
#    - Binary classification needs 0/1 labels, not one-hot
#    - Solution: labels = [0, 1, 1, 0, ...]
#
# 4. Vanishing gradients:
#    - LSTM helps but can still occur in very long sequences
#    - Solution: Use gradient clipping (add to train block)
#
# 5. Overfitting:
#    - Model memorizes training data
#    - Solution: Increase dropout, use early stopping
#
# ============================================================================
