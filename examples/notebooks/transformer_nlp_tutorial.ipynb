{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer-based NLP with Neural DSL\n",
    "\n",
    "This tutorial shows how to build transformer models for NLP tasks using Neural DSL.\n",
    "\n",
    "## Overview\n",
    "- Build a transformer encoder for text classification\n",
    "- Understand multi-head attention\n",
    "- Train on text datasets\n",
    "- Compare with LSTM models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from neural.parser.parser import create_parser, ModelTransformer\n",
    "from neural.code_generation.code_generator import generate_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsl_code = \"\"\"\n",
    "network TransformerNLP {\n",
    "  input: (None, 512)\n",
    "  \n",
    "  layers:\n",
    "    Embedding(input_dim=30000, output_dim=256)\n",
    "    Dropout(rate=0.1)\n",
    "    TransformerEncoder(num_heads=8, ff_dim=512, dropout=0.1)\n",
    "    TransformerEncoder(num_heads=8, ff_dim=512, dropout=0.1)\n",
    "    TransformerEncoder(num_heads=8, ff_dim=512, dropout=0.1)\n",
    "    GlobalAveragePooling1D()\n",
    "    Dense(units=256, activation=\"relu\")\n",
    "    Dropout(rate=0.3)\n",
    "    Dense(units=128, activation=\"relu\")\n",
    "    Dropout(rate=0.3)\n",
    "    Output(units=10, activation=\"softmax\")\n",
    "\n",
    "  loss: \"sparse_categorical_crossentropy\"\n",
    "  optimizer: Adam(learning_rate=0.0001)\n",
    "  metrics: [\"accuracy\"]\n",
    "\n",
    "  train {\n",
    "    epochs: 30\n",
    "    batch_size: 32\n",
    "    validation_split: 0.15\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "with open('transformer_nlp.neural', 'w') as f:\n",
    "    f.write(dsl_code)\n",
    "\n",
    "print(\"Transformer model defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Transformers\n",
    "\n",
    "The transformer architecture uses:\n",
    "- **Multi-head Attention**: Allows the model to focus on different positions\n",
    "- **Feed-forward Networks**: Process information after attention\n",
    "- **Layer Normalization**: Stabilizes training\n",
    "- **Positional Encoding**: Injects position information\n",
    "\n",
    "Key advantages over RNNs:\n",
    "- Parallel processing (faster training)\n",
    "- Better long-range dependencies\n",
    "- More interpretable attention patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!neural compile transformer_nlp.neural --backend tensorflow --output transformer_nlp_tf.py\n",
    "print(\"Model compiled!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!neural visualize transformer_nlp.neural --format html\n",
    "print(\"Visualization generated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data\n",
    "\n",
    "For this example, we'll use a text classification dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras.preprocessing import sequence\n",
    "    \n",
    "    # Load dataset (example with IMDB, can be replaced)\n",
    "    max_features = 30000\n",
    "    maxlen = 512\n",
    "    \n",
    "    print(\"Loading dataset...\")\n",
    "    (x_train, y_train), (x_test, y_test) = keras.datasets.imdb.load_data(\n",
    "        num_words=max_features\n",
    "    )\n",
    "    \n",
    "    # For multi-class, we'd need a different dataset\n",
    "    # Here we'll demonstrate with binary classification\n",
    "    \n",
    "    print(\"Padding sequences...\")\n",
    "    x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "    x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "    \n",
    "    print(f\"Training data shape: {x_train.shape}\")\n",
    "    print(f\"Test data shape: {x_test.shape}\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"TensorFlow not installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train using CLI\n",
    "!neural run transformer_nlp_tf.py --backend tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare with LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LSTM model for comparison\n",
    "lstm_code = \"\"\"\n",
    "network LSTMBaseline {\n",
    "  input: (None, 512)\n",
    "  \n",
    "  layers:\n",
    "    Embedding(input_dim=30000, output_dim=256)\n",
    "    LSTM(units=256, return_sequences=True, dropout=0.2)\n",
    "    LSTM(units=128, dropout=0.2)\n",
    "    Dense(units=128, activation=\"relu\")\n",
    "    Dropout(rate=0.3)\n",
    "    Output(units=10, activation=\"softmax\")\n",
    "\n",
    "  loss: \"sparse_categorical_crossentropy\"\n",
    "  optimizer: Adam(learning_rate=0.001)\n",
    "  metrics: [\"accuracy\"]\n",
    "\n",
    "  train {\n",
    "    epochs: 30\n",
    "    batch_size: 32\n",
    "    validation_split: 0.15\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "with open('lstm_baseline.neural', 'w') as f:\n",
    "    f.write(lstm_code)\n",
    "\n",
    "print(\"LSTM baseline model created for comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Attention Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pseudo-code for attention visualization\n",
    "# In practice, you'd extract attention weights from the model\n",
    "\n",
    "# import seaborn as sns\n",
    "# \n",
    "# def visualize_attention(attention_weights, tokens):\n",
    "#     plt.figure(figsize=(10, 8))\n",
    "#     sns.heatmap(attention_weights, xticklabels=tokens, yticklabels=tokens, \n",
    "#                 cmap='viridis', cbar=True)\n",
    "#     plt.title('Attention Weights')\n",
    "#     plt.xlabel('Key')\n",
    "#     plt.ylabel('Query')\n",
    "#     plt.show()\n",
    "\n",
    "print(\"Attention visualization code ready (requires model weights)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare training time and accuracy\n",
    "# results = {\n",
    "#     'Transformer': {'accuracy': 0.XX, 'time': XXX},\n",
    "#     'LSTM': {'accuracy': 0.XX, 'time': XXX}\n",
    "# }\n",
    "# \n",
    "# models = list(results.keys())\n",
    "# accuracies = [results[m]['accuracy'] for m in models]\n",
    "# times = [results[m]['time'] for m in models]\n",
    "# \n",
    "# fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "# \n",
    "# ax1.bar(models, accuracies)\n",
    "# ax1.set_ylabel('Accuracy')\n",
    "# ax1.set_title('Model Accuracy Comparison')\n",
    "# \n",
    "# ax2.bar(models, times)\n",
    "# ax2.set_ylabel('Training Time (s)')\n",
    "# ax2.set_title('Training Time Comparison')\n",
    "# \n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "print(\"Performance comparison template ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize hyperparameters\n",
    "!neural compile transformer_nlp.neural --backend tensorflow --hpo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug with NeuralDbg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"To debug, run:\")\n",
    "print(\"neural debug transformer_nlp.neural --backend tensorflow --dashboard --port 8050\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export to Production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to different formats\n",
    "!neural compile transformer_nlp.neural --backend pytorch --output transformer_pytorch.py\n",
    "!neural compile transformer_nlp.neural --backend onnx --output transformer.onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, we:\n",
    "1. Built a transformer-based NLP model\n",
    "2. Understood multi-head attention\n",
    "3. Compared with LSTM architectures\n",
    "4. Explored attention visualization\n",
    "\n",
    "## Next Steps\n",
    "- Build encoder-decoder transformers\n",
    "- Implement BERT-style pre-training\n",
    "- Try GPT-style autoregressive models\n",
    "- Fine-tune pre-trained transformers\n",
    "- Apply to machine translation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
