// Positional Encoding Example
// This example demonstrates the use of PositionalEncoding layer in transformer models

// Basic transformer with sinusoidal positional encoding
network BasicTransformer {
    input: (100, 512)
    layers:
        PositionalEncoding()
        TransformerEncoder(num_heads: 8, ff_dim: 2048)
        Output(10, "softmax")
    optimizer: "Adam" { learning_rate: 0.001 }
    loss: "categorical_crossentropy"
    training: {
        epochs: 10,
        batch_size: 32
    }
}

// Transformer with custom max_len
network CustomMaxLen {
    input: (200, 256)
    layers:
        PositionalEncoding(max_len: 1000)
        TransformerEncoder(num_heads: 4, ff_dim: 1024)
        Output(5, "softmax")
    optimizer: "Adam" { learning_rate: 0.0001 }
    loss: "categorical_crossentropy"
}

// Transformer with learnable positional encoding
network LearnablePositions {
    input: (50, 384)
    layers:
        PositionalEncoding(max_len: 512, encoding_type: "learnable")
        TransformerEncoder(num_heads: 6, ff_dim: 1536)
        Output(20, "softmax")
    optimizer: "Adam" { learning_rate: 0.0005 }
    loss: "categorical_crossentropy"
}

// Multi-layer transformer with positional encoding and dropout
network DeepTransformer {
    input: (128, 512)
    layers:
        PositionalEncoding(max_len: 2048, encoding_type: "sinusoidal")
        Dropout(0.1)
        TransformerEncoder(num_heads: 8, ff_dim: 2048, dropout: 0.1) * 6
        Output(100, "softmax")
    optimizer: "Adam" { learning_rate: 0.0001 }
    loss: "categorical_crossentropy"
    training: {
        epochs: 50,
        batch_size: 64,
        validation_split: 0.2
    }
}
